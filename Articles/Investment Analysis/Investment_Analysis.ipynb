{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahoo_fin.stock_info import *\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "import re\n",
    "from json import loads\n",
    "\n",
    "import talib as tb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm #Used in the for loops to track the progress of the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Data Folder\n",
    "Data_folder = os.path.abspath(os.getcwd() +'/Data/')\n",
    "if not os.path.exists(Data_folder):\n",
    "    os.makedirs(Data_folder)\n",
    "\n",
    "#Clean older files and folders in the Data Folder\n",
    "filelist = [ f for f in os.listdir(Data_folder)]\n",
    "for f in filelist:\n",
    "    shutil.rmtree(os.path.join(Data_folder, f), ignore_errors=True)\n",
    "\n",
    "filelist = [ f for f in os.listdir(Data_folder)]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join(Data_folder, f))\n",
    "\n",
    "#Create New Folder for Fundamental Analysis\n",
    "fundamental_analysis_folder = os.path.join(Data_folder, 'Fundamental Analysis')\n",
    "if not os.path.exists(fundamental_analysis_folder):\n",
    "    os.makedirs(fundamental_analysis_folder)\n",
    "    \n",
    "#Create New Folder for Analysts Recommendations\n",
    "analysts_recommendations_folder = os.path.join(Data_folder, 'Analysts Recommendations')\n",
    "if not os.path.exists(analysts_recommendations_folder):\n",
    "    os.makedirs(analysts_recommendations_folder)\n",
    "        \n",
    "#Create New Folder for Technical Analysis\n",
    "technical_analysis_folder = os.path.join(Data_folder, 'Technical Analysis')\n",
    "if not os.path.exists(technical_analysis_folder):\n",
    "    os.makedirs(technical_analysis_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert variables to number values\n",
    "def converter(variable):\n",
    "    convert_matrix = {'%': 1, 'K': 1000, 'M': 1000000, 'B': 1000000000, 'T': 1000000000000}\n",
    "    if pd.isnull(variable):\n",
    "        variable = 'nan'\n",
    "    elif type(variable) == float:\n",
    "        variable = variable\n",
    "    else:\n",
    "        units = variable[-1]\n",
    "        if (units == '%' or units == 'K' or units == 'M' or units == 'B' or units == 'T'):\n",
    "            variable = round(float(variable[:-1])*convert_matrix[units],2)\n",
    "        else:\n",
    "            variable = round(float(variable),2)\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain the Sector and Industry for one Ticker based on the Excel File: 'SP500_Index.csv'\n",
    "def GICS(ticker):\n",
    "    GICS_table = pd.read_csv('SP500_Index.csv', index_col = 'Ticker')\n",
    "    name = GICS_table[GICS_table.index==ticker]['Name'][0]\n",
    "    sector = GICS_table[GICS_table.index==ticker]['Sector'][0]\n",
    "    industry = GICS_table[GICS_table.index==ticker]['Industry'][0]\n",
    "    return name, sector, industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Tickers (Company Investment Symbol) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500 = ['MMM','ABT','ABBV','ABMD','ACN','ATVI','ADBE','AMD','AAP','AES','AMG','AFL','A','APD','AKAM','ALK','ALB','ARE','ALXN','ALGN','ALLE','AGN','ADS','LNT','ALL','GOOGL','MO','AMZN','AMCR','AEE','AAL','AEP','AXP','AIG','AMT','AWK','AMP','ABC','AME','AMGN','APH','ADI','ANSS','ANTM','AON','AOS','APA','AIV','AAPL','AMAT','APTV','ADM','ARNC','ANET','AJG','AIZ','ATO','T','ADSK','ADP','AZO','AVB','AVY','BHGE','BLL','BAC','BK','BAX','BBT','BDX','BRK-B','BBY','BIIB','BLK','HRB','BA','BKNG','BWA','BXP','BSX','BMY','AVGO','BR','BF-B','CHRW','COG','CDNS','CPB','COF','CPRI','CAH','KMX','CCL','CAT','CBOE','CBRE','CBS','CDW','CE','CELG','CNC','CNP','CTL','CERN','CF','SCHW','CHTR','CVX','CMG','CB','CHD','CI','XEC','CINF','CTAS','CSCO','C','CFG','CTXS','CLX','CME','CMS','KO','CTSH','CL','CMCSA','CMA','CAG','CXO','COP','ED','STZ','COO','CPRT','GLW','CTVA','COST','COTY','CCI','CSX','CMI','CVS','DHI','DHR','DRI','DVA','DE','DAL','XRAY','DVN','FANG','DLR','DFS','DISCA','DISCK','DISH','DG','DLTR','D','DOV','DOW','DTE','DUK','DRE','DD','DXC','ETFC','EMN','ETN','EBAY','ECL','EIX','EW','EA','EMR','ETR','EOG','EFX','EQIX','EQR','ESS','EL','EVRG','ES','RE','EXC','EXPE','EXPD','EXR','XOM','FFIV','FB','FAST','FRT','FDX','FIS','FITB','FE','FRC','FISV','FLT','FLIR','FLS','FMC','F','FTNT','FTV','FBHS','FOXA','FOX','BEN','FCX','GPS','GRMN','IT','GD','GE','GIS','GM','GPC','GILD','GL','GPN','GS','GWW','HAL','HBI','HOG','HIG','HAS','HCA','HCP','HP','HSIC','HSY','HES','HPE','HLT','HFC','HOLX','HD','HON','HRL','HST','HPQ','HUM','HBAN','HII','IEX','IDXX','INFO','ITW','ILMN','IR','INTC','ICE','IBM','INCY','IP','IPG','IFF','INTU','ISRG','IVZ','IPGP','IQV','IRM','JKHY','JEC','JBHT','SJM','JNJ','JCI','JPM','JNPR','KSU','K','KEY','KEYS','KMB','KIM','KMI','KLAC','KSS','KHC','KR','LB','LHX','LH','LRCX','LW','LVS','LEG','LDOS','LEN','LLY','LNC','LIN','LKQ','LMT','L','LOW','LYB','MTB','MAC','M','MRO','MPC','MKTX','MAR','MMC','MLM','MAS','MA','MKC','MXIM','MCD','MCK','MDT','MRK','MET','MTD','MGM','MCHP','MU','MSFT','MAA','MHK','TAP','MDLZ','MNST','MCO','MS','MOS','MSI','MSCI','MYL','NDAQ','NOV','NTAP','NFLX','NWL','NEM','NWSA','NWS','NEE','NLSN','NKE','NI','NBL','JWN','NSC','NTRS','NOC','NCLH','NRG','NUE','NVDA','NVR','ORLY','OXY','OMC','OKE','ORCL','PCAR','PKG','PH','PAYX','PYPL','PNR','PBCT','PEP','PKI','PRGO','PFE','PM','PSX','PNW','PXD','PNC','PPG','PPL','PFG','PG','PGR','PLD','PRU','PEG','PSA','PHM','PVH','QRVO','PWR','QCOM','DGX','RL','RJF','RTN','O','REG','REGN','RF','RSG','RMD','RHI','ROK','ROL','ROP','ROST','RCL','CRM','SBAC','SLB','STX','SEE','SRE','SHW','SPG','SWKS','SLG','SNA','SO','LUV','SPGI','SWK','SBUX','STT','SYK','STI','SIVB','SYMC','SYF','SNPS','SYY','TMUS','TROW','TTWO','TPR','TGT','TEL','FTI','TFX','TXN','TXT','TMO','TIF','TWTR','TJX','TSCO','TDG','TRV','TRIP','TSN','UDR','ULTA','USB','UAA','UA','UNP','UAL','UNH','UPS','URI','UTX','UHS','UNM','VFC','VLO','VAR','VTR','VRSN','VRSK','VZ','VRTX','VIAB','V','VNO','VMC','WAB','WMT','WBA','DIS','WM','WAT','WEC','WCG','WFC','WELL','WDC','WU','WRK','WY','WHR','WMB','WLTW','WYNN','XEL','XRX','XLNX','XYL','YUM','ZBH','ZION','ZTS']\n",
    "\n",
    "tickers = SP500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the tickers' list (tqdm gives the progress bar)\n",
    "for ticker in tqdm(tickers):\n",
    "    try:\n",
    "        #Obtain information on the Name, Sector and Industry of each Ticker\n",
    "        name, sector, industry = GICS(ticker)\n",
    "\n",
    "        #Get Tables from Yahoo: Key Stats Table and the Quote Table\n",
    "        key_stats = get_stats(ticker)\n",
    "        quote_table = get_quote_table(ticker)\n",
    "\n",
    "        try:\n",
    "            quote_table = get_quote_table(ticker)\n",
    "            Price = round(quote_table['Quote Price'],2)\n",
    "        except:\n",
    "            Price = yahoo_financials.get_current_price()\n",
    "\n",
    "        # Getting the Market Cap: Identifying biggest companies\n",
    "        Market_Cap = converter(key_stats['Value'][0])\n",
    "\n",
    "        #Fundamental Analysis Multiples: Price Multiples\n",
    "        P = Price\n",
    "        PE = converter(key_stats['Value'][2]) if converter(key_stats['Value'][2]) != 'nan' else converter(key_stats['Value'][3])\n",
    "        PEG = converter(key_stats['Value'][4]) if converter(key_stats['Value'][4]) != 'nan' else 0\n",
    "        PB = converter(key_stats['Value'][6]) if converter(key_stats['Value'][6]) != 'nan' else 0\n",
    "\n",
    "        #Fundamental Analysis Multiples: Enterprise Value (EV) Multiples\n",
    "        EV_EBITDA = converter(key_stats['Value'][8]) if converter(key_stats['Value'][8]) != 'nan' else 0\n",
    "\n",
    "        ##EV to Sales is not available in Key Stats table so we will need to construct it\n",
    "        EV = converter(key_stats['Value'][1]) if converter(key_stats['Value'][1]) != 'nan' else 0\n",
    "        P_Sales = converter(key_stats['Value'][5]) if converter(key_stats['Value'][5]) != 'nan' else 0\n",
    "        Sales = Market_Cap/P_Sales if P_Sales != 0 else 0\n",
    "        EV_Sales = round(EV / Sales,2)\n",
    "\n",
    "        #Construct Ticker Table to Print\n",
    "        fundamental_data_table = pd.DataFrame(data={\n",
    "            'Ticker': [ticker], 'Name': [name], 'Sector': [sector], 'Industry': [industry], 'Price': [P], 'PE': [PE], \n",
    "            'PEG': [PEG], 'PB': [PB], 'EV_EBITDA': [EV_EBITDA], 'EV_Sales': [EV_Sales]\n",
    "        }).set_index('Ticker')\n",
    "        ##Save Table\n",
    "        fundamental_data_table.to_csv(os.path.join(fundamental_analysis_folder,ticker)+'_Table.csv')\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join All the Ticker Data Files into a Table\n",
    "files= os.listdir(fundamental_analysis_folder)\n",
    "\n",
    "dfList = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(fundamental_analysis_folder,file)) \n",
    "    df.set_index('Ticker', inplace=True, drop=True)\n",
    "\n",
    "    dfList.append(df)\n",
    "\n",
    "fundamental_data_table = pd.concat(dfList, axis=0)\n",
    "\n",
    "fundamental_data_table.to_csv('Ticker_Data_Table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Analysis: Calculate the Industry Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the complete Ticker Data to include Industry metrics\n",
    "fundamental_data_table = pd.read_csv('Ticker_Data_Table.csv')\n",
    "\n",
    "    #Calculate Industry metrics\n",
    "Ind_PE = fundamental_data_table.groupby('Industry')['PE'].mean()\n",
    "Ind_PEG = fundamental_data_table.groupby('Industry')['PEG'].mean()\n",
    "Ind_PB = fundamental_data_table.groupby('Industry')['PB'].mean()\n",
    "Ind_EV_EBITDA = fundamental_data_table.groupby('Industry')['EV_EBITDA'].mean()\n",
    "Ind_EV_Sales = fundamental_data_table.groupby('Industry')['EV_Sales'].mean()\n",
    "\n",
    "    #Include Industry metrics in the Full table\n",
    "fundamental_data_table['Ind_PE'] = pd.merge(fundamental_data_table['Industry'], Ind_PE, on = 'Industry', how = 'left').iloc[:,1]\n",
    "fundamental_data_table['Ind_PEG'] = pd.merge(fundamental_data_table['Industry'], Ind_PEG, on = 'Industry', how = 'left').iloc[:,1]\n",
    "fundamental_data_table['Ind_PB'] = pd.merge(fundamental_data_table['Industry'], Ind_PB, on = 'Industry', how = 'left').iloc[:,1]\n",
    "fundamental_data_table['Ind_EV_EBITDA'] = pd.merge(fundamental_data_table['Industry'], Ind_EV_EBITDA, on = 'Industry', how = 'left').iloc[:,1]\n",
    "fundamental_data_table['Ind_EV_Sales'] = pd.merge(fundamental_data_table['Industry'], Ind_EV_Sales, on = 'Industry', how = 'left').iloc[:,1]\n",
    "\n",
    "#Save Final Ticker Table to File\n",
    "fundamental_data_table = fundamental_data_table.set_index('Ticker')\n",
    "fundamental_data_table = fundamental_data_table[['Name', 'Sector', 'Industry', 'Price', 'PE', 'Ind_PE', 'PEG', 'Ind_PEG', \n",
    "                                                 'PB', 'Ind_PB', 'EV_EBITDA', 'Ind_EV_EBITDA', 'EV_Sales','Ind_EV_Sales']]\n",
    "fundamental_data_table = fundamental_data_table.round(2)\n",
    "fundamental_data_table.to_csv(os.path.join(Data_folder,'Fundamental_Data_Table.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyst Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the tickers' list (tqdm gives the progress bar)\n",
    "for ticker in tqdm(tickers):\n",
    "    try:\n",
    "        #Obtain information on the Name, Sector and Industry of each Ticker\n",
    "        name, sector, industry = GICS(ticker)\n",
    "        \n",
    "        #Obtain the Current Price\n",
    "        quote_table = get_quote_table(ticker)\n",
    "        \n",
    "        #Webscrapping the Data in Yahoo\n",
    "        r = requests.get('https://finance.yahoo.com/quote/'+ticker+'/analysis?p='+ticker)\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        script = soup.find(\"script\",text=re.compile(\"root.App.main\")).text\n",
    "        data = loads(re.search(\"root.App.main\\s+=\\s+(\\{.*\\})\", script).group(1))\n",
    "        \n",
    "        #Obatining the Analysts' Recommendation, Number of Options and Price Target\n",
    "        Analyst_recomm = data[\"context\"][\"dispatcher\"][\"stores\"]['QuoteSummaryStore']['financialData']['recommendationKey']\n",
    "        Analysts_number = data[\"context\"][\"dispatcher\"][\"stores\"]['QuoteSummaryStore']['financialData']['numberOfAnalystOpinions']['raw']\n",
    "        P_Target = quote_table['1y Target Est']\n",
    "        \n",
    "        try:\n",
    "            quote_table = get_quote_table(ticker)\n",
    "            P = round(quote_table['Quote Price'],2)\n",
    "        except:\n",
    "            P = yahoo_financials.get_current_price()\n",
    "\n",
    "        #Obatin the Probability of Upside from the Target Price\n",
    "        Prob_Up = round((P_Target/P-1)*100,2)\n",
    "        \n",
    "        #Construct Ticker Table to Print\n",
    "        analysts_data_table = pd.DataFrame(data={\n",
    "            'Ticker': [ticker], 'Name': [name], 'Sector': [sector], 'Industry': [industry],'Price': [P], 'P_Target': [P_Target], \n",
    "            'Prob_Up %': [Prob_Up], 'Analyst_recomm': [Analyst_recomm], 'Analysts_number': [Analysts_number], \n",
    "        }).set_index('Ticker')\n",
    "        ##Save Table\n",
    "        analysts_data_table.to_csv(os.path.join(analysts_recommendations_folder,ticker)+'_Table.csv')\n",
    "                \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join All the Ticker Data Files into a Table\n",
    "files= os.listdir(analysts_recommendations_folder)\n",
    "\n",
    "dfList = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(analysts_recommendations_folder,file)) \n",
    "    df.set_index('Ticker', inplace=True, drop=True)\n",
    "\n",
    "    dfList.append(df)\n",
    "\n",
    "analysts_data_table = pd.concat(dfList, axis=0)\n",
    "analysts_data_table = analysts_data_table.round(2)\n",
    "\n",
    "analysts_data_table.to_csv(os.path.join(Data_folder,'Analysts_Data_Table.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price Data: Defining the Start and End dates \n",
    "finishing_period = time.strftime('%d/%m/%Y')\n",
    "\n",
    "    ##Let's define the start date as 1 year ago\n",
    "beginning_period_unformat = datetime.datetime.strptime(finishing_period, '%d/%m/%Y') - datetime.timedelta(days=366)\n",
    "beginning_period = beginning_period_unformat.strftime('%d/%m/%Y')\n",
    "\n",
    "for ticker in tqdm(tickers):\n",
    "    try:\n",
    "        #Obtain information on the Name, Sector and Industry of each Ticker\n",
    "        name, sector, industry = GICS(ticker)\n",
    "        \n",
    "        #Price Data: Defining the Start and End dates \n",
    "        finishing_period = time.strftime('%m/%d/%Y')\n",
    "\n",
    "        ##Let's define the start date as 1 year ago\n",
    "        beginning_period_unformat = datetime.datetime.strptime(finishing_period, '%m/%d/%Y') - datetime.timedelta(days=366)\n",
    "        beginning_period = beginning_period_unformat.strftime('%m/%d/%Y')\n",
    "\n",
    "        historical_data = get_data(ticker, start_date = beginning_period , end_date = finishing_period)\n",
    "\n",
    "        #Let's Create a DataFrame with the downloaded Data\n",
    "        df = pd.DataFrame(historical_data, columns=['open', 'high', 'low', 'close', 'adjclose', 'volume'])\n",
    "        df['date'] = pd.to_datetime(df.index, format='%Y/%m/%d')\n",
    "\n",
    "        #Let's define the Variables for the OHLCV metrics\n",
    "        open_price = df.open.values\n",
    "        high_price = df.high.values\n",
    "        low_price = df.low.values\n",
    "        close_price = df.close.values\n",
    "        volume_moment = np.array(df.volume.values, dtype='f8')\n",
    "\n",
    "        #Indicators Calculation\n",
    "        PSAR = tb.SAR(high= high_price, low= low_price, acceleration=0, maximum=0)\n",
    "        \n",
    "        MA_200d = tb.SMA(close_price, timeperiod=200)\n",
    "        MA_50d = tb.SMA(close_price, timeperiod=50)\n",
    "        Golden_Cross = MA_50d - MA_200d\n",
    "\n",
    "        RSI = tb.RSI(close_price, timeperiod=14)\n",
    "        CCI= tb.CCI(high= high_price, low= low_price, close= close_price, timeperiod=14)\n",
    "\n",
    "        ADX = tb.ADX(high= high_price, low= low_price, close= close_price, timeperiod=14)\n",
    "        DI_plus = tb.PLUS_DI(high= high_price, low= low_price, close= close_price, timeperiod=14)\n",
    "        DI_minus = tb.MINUS_DI(high= high_price, low= low_price, close= close_price, timeperiod=14)\n",
    "        DI_index = DI_plus - DI_minus\n",
    "\n",
    "        #Construct Ticker Table to Print\n",
    "        technical_data_table = pd.DataFrame(data={\n",
    "            'date': df['date'],'Ticker': ticker, 'Name': name, 'Sector': sector, 'Industry': industry,\n",
    "            'Price': close_price, 'PSAR': PSAR, 'Golden_Cross': Golden_Cross, 'RSI': RSI, 'CCI': CCI, \n",
    "            'ADX': ADX, 'DI_index': DI_index, 'Chaikin_Osc': Chaikin_Osc,\n",
    "        }).set_index('date')\n",
    "\n",
    "        technical_data_table = technical_data_table.reset_index()\n",
    "        technical_data_table.sort_values(by='date', inplace=True, ascending=False)\n",
    "        technical_data_table = technical_data_table.set_index('date')\n",
    "        technical_data_table.to_csv(os.path.join(technical_analysis_folder,ticker)+'_Table.csv')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join All the Ticker Data Files into a Table\n",
    "files= os.listdir(technical_analysis_folder)\n",
    "\n",
    "dfList = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(technical_analysis_folder,file)) \n",
    "    df_final = df[:1]\n",
    "    df_final = df_final.drop(columns=['date'])\n",
    "    df_final.set_index('Ticker', inplace=True, drop=True)\n",
    "\n",
    "    dfList.append(df_final)\n",
    "\n",
    "technical_data_table = pd.concat(dfList, axis=0)\n",
    "technical_data_table = technical_data_table.round(2)\n",
    "\n",
    "technical_data_table.to_csv(os.path.join(Data_folder,'Technical_Data_Table.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the DataFrames to html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, save\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.widgets import TableColumn, DataTable\n",
    "from bokeh.layouts import row, widgetbox\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "\n",
    "import panel as pn\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Fundamental Analysis table to html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_data_table = pd.read_csv(os.path.join(Data_folder,'Fundamental_Data_Table.csv'))\n",
    "\n",
    "#Get a list of all Sectors and Industries, and include one entry for \"All\"\n",
    "sectors = fundamental_data_table.Sector.unique()\n",
    "sectors = np.append('All',sectors)\n",
    "\n",
    "industries = fundamental_data_table.Industry.unique()\n",
    "industries = np.append('All',industries)\n",
    "\n",
    "#Instantiate a Table and define the Table Generation dynamics\n",
    "##Note that the \"if's\" were introduced to account for the \"All\" values in Sectors and Industries as no stock has this value \n",
    "def load_stocks(Sector, Industry):\n",
    "    if Sector == 'All' and Industry == 'All':\n",
    "        table = hv.Table(fundamental_data_table)    \n",
    "    elif Sector == 'All':\n",
    "        table = hv.Table(fundamental_data_table[fundamental_data_table.Industry.isin([Industry])])\n",
    "    elif Industry == 'All':\n",
    "        table = hv.Table(fundamental_data_table[fundamental_data_table.Sector.isin([Sector])])    \n",
    "    else:\n",
    "        table = hv.Table(fundamental_data_table[fundamental_data_table.Sector.isin([Sector]) & fundamental_data_table.Industry.isin([Industry])])    \n",
    "    table = table.opts(opts.Table(width=950, height=280, selectable = True, index_position = None))\n",
    "    return table\n",
    "\n",
    "#Instantiate the DynamicMap function, so to generate the Table defined with the Widgets for Sectors and Industries\n",
    "dmap = hv.DynamicMap(load_stocks, kdims=['Sector','Industry']).redim.values(Sector=sectors, Industry=industries)\n",
    "dmap = dmap.opts(framewise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the Panel library to be able to save the Table generated\n",
    "p = pn.panel(dmap,  widget_location='top_left')\n",
    "p.save('Fundamental_Analysis_table.html', embed = True, max_states=1536) \n",
    "##Note that max_states had to be introduce since it's over 1000, otherwise there's no need to include it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Analysts Recommendations table to html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysts_data_table = pd.read_csv(os.path.join(Data_folder,'Analysts_Data_Table.csv'))\n",
    "\n",
    "#Get a list of all the Recommendations' types, and include one entry for \"All\"\n",
    "recommendations = analysts_data_table.Analyst_recomm.unique()\n",
    "recommendations = np.append('All',recommendations)\n",
    "idx = [0, 4, 1, 2, 3,5]\n",
    "recommendations = recommendations[idx]\n",
    "\n",
    "#Instantiate a Table and define the Table Generation dynamics\n",
    "##Note that the \"if\" was introduced to account for the \"All\" values in Recommendations as no stock has this value \n",
    "def load_stocks(Recommendation):\n",
    "    if Recommendation == 'All':\n",
    "        table = hv.Table(analysts_data_table)       \n",
    "    else:\n",
    "        table = hv.Table(analysts_data_table[analysts_data_table.Analyst_recomm.isin([Recommendation])])    \n",
    "    table = table.opts(opts.Table(width=950, height=280, selectable = True, index_position = None))\n",
    "    return table\n",
    "\n",
    "#Instantiate the DynamicMap function, so to generate the Table defined with the Widget for Recommendation\n",
    "dmap = hv.DynamicMap(load_stocks, kdims=['Recommendation']).redim.values(Recommendation=recommendations)\n",
    "dmap = dmap.opts(framewise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the Panel library to be able to save the Table generated\n",
    "p = pn.panel(dmap,  widget_location='top_left')\n",
    "p.save('Analysts_Recommendation_table.html', embed = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Technical Analysis table to html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_data_table = pd.read_csv(os.path.join(Data_folder,'Technical_Data_Table.csv'))\n",
    "\n",
    "#Get a maximum and minimum values for the CCI and ADX\n",
    "CCI_min_abs = round(technical_data_table['CCI'].min() -5,-1)\n",
    "CCI_max_abs = round(technical_data_table['CCI'].max() +5,-1)\n",
    "\n",
    "ADX_min_abs = round(technical_data_table['ADX'].min() -5, -1)\n",
    "ADX_max_abs = round(technical_data_table['ADX'].max() +5, -1)\n",
    "#Note that the other Sliders will have the standard range:\n",
    "    ## RSI: 0 to 100\n",
    "\n",
    "#Instantiate a Table and define the Table Generation dynamics\n",
    "def load_stocks(RSI_min, RSI_max, CCI_min, CCI_max, ADX_value): \n",
    "    table = hv.Table(technical_data_table[(technical_data_table['RSI'].between(RSI_min,RSI_max)) & \n",
    "                                          (technical_data_table['CCI'].between(CCI_min,CCI_max)) &\n",
    "                                          (technical_data_table['ADX'] > ADX_value)                                           \n",
    "                                         ]).opts(opts.Table(width=850, index_position = None))\n",
    "    return table\n",
    "\n",
    "#Instantiate the DynamicMap function, so to generate the Table defined with the Widgets for RSI, CCI and ADX\n",
    "dmap = hv.DynamicMap(load_stocks, kdims=['RSI_min','RSI_max', 'CCI_min','CCI_max', 'ADX_value']).redim.range(\n",
    "    RSI_min=(0, 100), RSI_max=(0, 100), CCI_min=(CCI_min_abs, CCI_max_abs), CCI_max=(CCI_min_abs, CCI_max_abs), \n",
    "    ADX_value=(ADX_min_abs, ADX_max_abs))\n",
    "dmap = dmap.opts(framewise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the Panel library to be able to adjust the Widgets and save the Table generated\n",
    "p = pn.pane.HoloViews(dmap, widgets={\n",
    "    'RSI_min': pn.widgets.DiscreteSlider(name='RSI_min', options=np.arange(0, 101, 25).tolist(), value=0, width=150),\n",
    "    'RSI_max': pn.widgets.DiscreteSlider(name='RSI_max', options=np.arange(0, 101, 25).tolist(), value=100, width=150),\n",
    "    'CCI_min': pn.widgets.DiscreteSlider(name='CCI_min', options=[CCI_min_abs, -100, 0, 100, CCI_max_abs], value=CCI_min_abs, width=150),\n",
    "    'CCI_max': pn.widgets.DiscreteSlider(name='CCI_max', options=[CCI_min_abs, -100, 0, 100, CCI_max_abs], value=CCI_max_abs, width=150),\n",
    "    'ADX_value': pn.widgets.DiscreteSlider(name='ADX_value', options=np.arange(ADX_min_abs, ADX_max_abs, 10).tolist(), value=ADX_min_abs, width=150)},\n",
    "                 ).layout\n",
    "p.save('Technical_Analysis_table.html', embed = True, max_states = 3750)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
